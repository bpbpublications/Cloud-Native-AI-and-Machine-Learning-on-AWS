{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update SageMaker libraries\n",
    "!pip3 install -U sagemaker --disable-pip-version-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import uuid\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Enter a bucket name - you can use the same bucket you used in other chapters\n",
    "bucket = 'your-S3-bucket-name'\n",
    "prefix = 'aiml-book/chapter9'\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to dataframe\n",
    "raw_df = pd.read_csv('kaggle-house-prices-dataset.csv', header=0)\n",
    "# drop columns containing null values\n",
    "raw_df.dropna(axis=1, inplace=True)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost algorithm considers the **first feature as the predicted or label feature**. In our example, we are training a model to predict the **SalePrice** of a house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a low dimensional dataset with a few numeric and categorical features for our example\n",
    "small_df = raw_df[['SalePrice','LotArea','Street','LotShape','LandContour','LotConfig','YrSold','SaleType','SaleCondition']]\n",
    "small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform numerical encoding of categorical features\n",
    "encoded_df = pd.get_dummies(small_df)\n",
    "encoded_df.head()\n",
    "# you can see from the results below that one hot encoding was applied for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 90% as train data and 10% as test data\n",
    "train_index=int(0.9 * len(encoded_df))\n",
    "train_df = encoded_df.iloc[:train_index,:]\n",
    "test_df = encoded_df.iloc[train_index:,:]\n",
    "# remove the label feature from the test dataset\n",
    "test_df_no_label = test_df.drop(['SalePrice'], axis=1)\n",
    "print(\"Train dataset dimensions: \" + str(train_df.shape))\n",
    "print(\"Test dataset dimensions: \" + str(test_df_no_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV files and upload to S3 bucket\n",
    "train_df.to_csv('train.csv',index=False, header=False)\n",
    "test_df_no_label.to_csv('test.csv',index=False, header=False)\n",
    "s3.upload_file('train.csv',bucket,prefix+'/train/train.csv')\n",
    "s3.upload_file('test.csv',bucket,prefix+'/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training input for SageMaker model training\n",
    "train_input = TrainingInput('s3://{}/{}/{}/'.format(bucket, prefix, 'train'), content_type='csv')\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Model Training\n",
    "\n",
    "Please refer to SageMaker documentation for descriptions of hyperparameters - https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let us define the hyperparameters\n",
    "xgboost_hps = {\n",
    "        \"max_depth\":\"6\",\n",
    "        \"eta\":\"0.3\",\n",
    "        \"gamma\":\"2\",\n",
    "        \"min_child_weight\":\"4\",\n",
    "        \"subsample\":\"0.5\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"30\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let us get the XGBoost built-in image\n",
    "xgb_image = sagemaker.image_uris.retrieve(\"xgboost\", 'us-east-1', \"1.5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path to store model artifacts\n",
    "model_prefix = prefix+'/xgboost-model'\n",
    "model_output = 's3://{}/{}/output'.format(bucket, model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build XGBoost estimator\n",
    "xgb_estimator = Estimator(image_uri=xgb_image, \n",
    "                            hyperparameters=xgboost_hps,\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            instance_count=1, \n",
    "                            instance_type='ml.m5.2xlarge', \n",
    "                            volume_size=10, # 10 GB \n",
    "                            output_path=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the estimator to run training\n",
    "xgb_estimator.fit({'train':train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can directly use the Estimator object to create a model and deploy it to an endpoint, for the purposes of this example, we will use the boto3 APIs to show how to create a model, create an endpoint configuration and finally create an endpoint and deploy the model. This decoupling will show you how you can simple bring a trained model and directly host it on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time endpoint single-container single-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model artifacts are stored here\n",
    "print(xgb_estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SageMaker boto3 handle\n",
    "sm = boto3.client('sagemaker')\n",
    "# our model name\n",
    "model_name = 'chapter9-xgboost-model-one-container1'\n",
    "# now let us create a model based on the trained model artifacts\n",
    "model_res = sm.create_model(\n",
    "                ModelName = model_name,\n",
    "                ExecutionRoleArn = sagemaker.get_execution_role(),\n",
    "                PrimaryContainer = {\n",
    "                    'Image': xgb_image,\n",
    "                    'ModelDataUrl': xgb_estimator.model_data,\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_res['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_config_name = model_name +'-epconfig'\n",
    "epcfg_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=ep_config_name, \n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"chapter9-test-variant\", # The name of the production variant.\n",
    "            \"ModelName\": model_name, \n",
    "            \"InstanceType\": 'ml.m5.xlarge', # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint Configuration successfully created: \" + epcfg_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_name = model_name+'-ep' \n",
    "ep_response = sm.create_endpoint(EndpointName=ep_name, EndpointConfigName=ep_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the print statement here shows InService\n",
    "print(sm.describe_endpoint(EndpointName=ep_name)['EndpointStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buffer for the csv request data from our test dataset\n",
    "from io import StringIO\n",
    "inf_req = StringIO()\n",
    "test_df_no_label.to_csv(inf_req,header=False, index=False)\n",
    "# if you want to check if your buffer is created correctly, uncomment and execute the below line\n",
    "#print(inf_req.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a runtime handler for SageMaker\n",
    "sm_run = boto3.client(\"sagemaker-runtime\")\n",
    "# now call the endpoint\n",
    "predictions = sm_run.invoke_endpoint(\n",
    "                            EndpointName=ep_name, \n",
    "                            Body=inf_req.getvalue(), # the values from the StringIO buffer we created in the previous cell\n",
    "                            ContentType='text/csv'\n",
    "                            )\n",
    "#check if we getproper response - the predicted sale prices\n",
    "print(predictions['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time endpoint Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SageMaker boto3 handle\n",
    "sm = boto3.client('sagemaker')\n",
    "# our model name\n",
    "model_name_serverless = 'chapter9-xgboost-model-serverless'\n",
    "# now let us create a model based on the trained model artifacts. For serverless we will use the Containers list rather than the PrimaryContainer\n",
    "model_res_serverless = sm.create_model(\n",
    "                ModelName = model_name_serverless,\n",
    "                ExecutionRoleArn = sagemaker.get_execution_role(),\n",
    "                Containers = [{\n",
    "                    'Image': xgb_image,\n",
    "                    'Mode': 'SingleModel',\n",
    "                    'ModelDataUrl': xgb_estimator.model_data,\n",
    "                }]\n",
    "            )\n",
    "print(model_res_serverless['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Serverless Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_config_name_serverless = model_name_serverless +'-epconfig'\n",
    "epcfg_response_serverless = sm.create_endpoint_config(\n",
    "    EndpointConfigName=ep_config_name_serverless, \n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'chapter9-serverless', \n",
    "            'ModelName': model_name_serverless, \n",
    "            'ServerlessConfig': {\n",
    "                \"MemorySizeInMB\": 3072,\n",
    "                \"MaxConcurrency\": 25\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Serverless Endpoint Configuration successfully created: \" + epcfg_response_serverless['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Serverless Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the endpoint\n",
    "ep_name_serverless = model_name_serverless+'-ep' \n",
    "ep_response_serverless = sm.create_endpoint(EndpointName=ep_name_serverless, EndpointConfigName=ep_config_name_serverless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the print statement here shows InService - should take 3 to 5 mins\n",
    "print(sm.describe_endpoint(EndpointName=ep_name_serverless)['EndpointStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Serverless Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a buffer for the csv request data from our test dataset\n",
    "from io import StringIO\n",
    "inf_req_svl = StringIO()\n",
    "test_df_no_label.to_csv(inf_req_svl,header=False, index=False)\n",
    "# if you want to check if your buffer is created correctly, uncomment and execute the below line\n",
    "#print(inf_req.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a runtime handler for SageMaker\n",
    "sm_run = boto3.client(\"sagemaker-runtime\")\n",
    "# now call the endpoint\n",
    "serverless_predictions = sm_run.invoke_endpoint(\n",
    "                            EndpointName=ep_name, \n",
    "                            Body=inf_req_svl.getvalue(), # the values from the StringIO buffer we created in the previous cell\n",
    "                            ContentType='text/csv'\n",
    "                            )\n",
    "#check if we getproper response - the predicted sale prices\n",
    "print(serverless_predictions['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with SageMaker Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 location for our test dataset\n",
    "s3_test = 's3://{}/{}/{}'.format(bucket, prefix, 'test/test.csv')\n",
    "s3_batch_out = 's3://{}/{}/{}'.format(bucket, prefix, 'batch/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input details for the Batch Transform\n",
    "transform_input = {\n",
    "    'DataSource': {\n",
    "        'S3DataSource': {\n",
    "            'S3DataType':'S3Prefix',\n",
    "            'S3Uri':s3_test\n",
    "        }\n",
    "    },\n",
    "    'ContentType':'text/csv',\n",
    "    'SplitType':'Line'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location for storing batch outputs\n",
    "transform_output = {\n",
    "    'S3OutputPath':s3_batch_out,\n",
    "    'AssembleWith':'Line'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure compute for the batch transform\n",
    "transform_resources = {\n",
    "    'InstanceType':'ml.m5.2xlarge',\n",
    "    'InstanceCount': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the batch transform job\n",
    "batch_job_name = 'chapter9-batch-inference'\n",
    "batch_res = sm.create_transform_job(\n",
    "                TransformJobName=batch_job_name,\n",
    "                ModelName=model_name,\n",
    "                MaxPayloadInMB=1,\n",
    "                BatchStrategy='MultiRecord',\n",
    "                TransformInput=transform_input,\n",
    "                TransformOutput=transform_output,\n",
    "                TransformResources=transform_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = sm.describe_transform_job(TransformJobName=batch_job_name)\n",
    "print(\"Job Name is: \"+job_details['TransformJobName'])\n",
    "print(\"Job Status is: \"+ job_details['TransformJobStatus'])\n",
    "print(\"Model Name is: \"+ job_details['ModelName'])\n",
    "print(\"Job results will be available in: \"+ job_details['TransformOutput']['S3OutputPath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the outputs from the batch transform job\n",
    "out_file = 'test.csv.out'\n",
    "s3.download_file(bucket, prefix+'/batch/output/'+out_file, out_file)\n",
    "out_df = pd.read_csv(out_file,header=None)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to build and use endpoints for hosting, adding an Elastic inference is really easy. All you have to do is compile our model using SageMaker Neo, and specify an accelerator of type Elastic Inference when you create your endpoint configuration. We will see how to do this in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a pre-trained TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from tensorflow import keras\n",
    "# import resnet50\n",
    "resnet_model = keras.applications.resnet50.ResNet50(weights='imagenet', include_top=True)\n",
    "# save model and create a tar.gz that SageMaker needs to create the Tensorflow estimator\n",
    "m_dir = '1'\n",
    "tf.saved_model.save(resnet_model,m_dir)\n",
    "# open a tar file and save model contents\n",
    "with tarfile.open('model.tar.gz','w:gz') as entry:\n",
    "    entry.add(m_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_path = 'tensorflow/model/model.tar.gz'\n",
    "tf_s3_path = 's3://{}/{}/{}'.format(bucket,prefix,tf_path)\n",
    "s3.upload_file('model.tar.gz',bucket,prefix+'/'+tf_path)\n",
    "# Create a Tensorflow estimator reference from the model\n",
    "tf_model = TensorFlowModel(model_data=tf_s3_path, framework_version='2.3', role=sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deploy it to a SageMaker endpoint with an Elastic Inference accelerator attached we simply pass this to the deploy method\n",
    "tf_endpoint = tf_model.deploy(instance_type='ml.m5.xlarge', initial_instance_count=1, accelerator_type=\"ml.eia2.medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction of a cat image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "cat_pic = load_img('phoebe.PNG', target_size=(224,224))\n",
    "plt.imshow(cat_pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = img_to_array(cat_pic)\n",
    "arr_bat = np.expand_dims(np_arr, axis=0)\n",
    "arr_bat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the image for prediction\n",
    "b4_pred_img = keras.applications.resnet50.preprocess_input(arr_bat.copy())\n",
    "# make predictions and decode the output to a class\n",
    "results = tf_endpoint.predict({\"inputs\": b4_pred_img.tolist()})\n",
    "# convert to numpy array\n",
    "new_res = np.array(results['outputs'])\n",
    "# Get class predictions for the picture\n",
    "print(keras.applications.imagenet_utils.decode_predictions(new_res))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-gpu-py38-cu112-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
