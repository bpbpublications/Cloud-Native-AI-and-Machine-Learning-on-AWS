{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learning does not make one learned there are those who have knowledge and those who have understanding The first requires memory and the second philosophy\n",
      " For all evils there are two remedies time and silence\n",
      " and as imagination bodies forth the forms of things unknown the poet pen turns them to shape and gives to airy nothing a local habitation and a name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the nltk library after you have installed it\n",
    "import nltk\n",
    "# download the library subset that is needed for word tokenization\n",
    "nltk.download('punkt')\n",
    "# import the word tokenization function\n",
    "from nltk.tokenize import word_tokenize\n",
    "# creating a list of our quotes from the input dataset\n",
    "quotes = [\"Learning does not make one learned: there are those who have knowledge and those who have understanding. The first requires memory and the second philosophy.\",\"For all evils there are two remedies - time and silence.\",\"and as imagination bodies forth the forms of things unknown, the poet's pen turns them to shape, and gives to airy nothing a local habitation and a name\"]\n",
    "# define a list for feature engineering output\n",
    "fe1_quotes = []\n",
    "for quote in quotes:\n",
    "    # get the word tokens from each quote\n",
    "    r_words = word_tokenize(quote)\n",
    "    # remove punctuations\n",
    "    p_words = [w for w in r_words if w.isalpha()]\n",
    "    p_removed = ''\n",
    "    # put the works with punctuations removed back into a sentence\n",
    "    for p_word in p_words:\n",
    "        p_removed += ' ' + p_word\n",
    "    fe1_quotes.append(p_removed)\n",
    "    print(p_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' learning does not make one learned there are those who have knowledge and those who have understanding the first requires memory and the second philosophy', ' for all evils there are two remedies time and silence', ' and as imagination bodies forth the forms of things unknown the poet pen turns them to shape and gives to airy nothing a local habitation and a name']\n"
     ]
    }
   ],
   "source": [
    "fe2_quotes = []\n",
    "for quote in fe1_quotes:\n",
    "    fe2_quotes.append(quote.lower())\n",
    "print(fe2_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' learning make one learned knowledge understanding first requires memory second philosophy', ' evils two remedies time silence', ' imagination bodies forth forms things unknown poet pen turns shape gives airy nothing local habitation name']\n"
     ]
    }
   ],
   "source": [
    "# download the stopwords library\n",
    "nltk.download('stopwords')\n",
    "# import the word tokenization function\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import the stop words function\n",
    "from nltk.corpus import stopwords\n",
    "# Define a new list for storing quotes with the stop words removed\n",
    "fe3_quotes = []\n",
    "# get the full list of stop words in the English language\n",
    "stop_ws = set(stopwords.words('english'))\n",
    "for quote in fe2_quotes:\n",
    "    words = word_tokenize(quote)\n",
    "    s_words = [word for word in words if not word in stop_ws]\n",
    "    s_removed = ''\n",
    "    for s_word in s_words:\n",
    "        s_removed += ' ' + s_word\n",
    "    fe3_quotes.append(s_removed)\n",
    "print(fe3_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' learn make one learn knowledg understand first requir memori second philosophi', ' evil two remedi time silenc', ' imagin bodi forth form thing unknown poet pen turn shape give airi noth local habit name']\n"
     ]
    }
   ],
   "source": [
    "# import the stemming function\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# declare the porter stemmer, the most commonly used stemming function\n",
    "porter = PorterStemmer()\n",
    "fe4_quotes = []\n",
    "for quote in fe3_quotes:\n",
    "    words = word_tokenize(quote)\n",
    "    # stem each of the words for each of our quotes\n",
    "    stem_words = [porter.stem(word) for word in words]\n",
    "    stem_quote = ''\n",
    "    for stem_word in stem_words:\n",
    "        stem_quote += ' ' + stem_word\n",
    "    fe4_quotes.append(stem_quote)\n",
    "print(fe4_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' learning make one learned knowledge understanding first requires memory second philosophy', ' evil two remedy time silence', ' imagination body forth form thing unknown poet pen turn shape give airy nothing local habitation name']\n"
     ]
    }
   ],
   "source": [
    "# download the wordnet library for lemmatizer\n",
    "nltk.download('wordnet')\n",
    "# import the lemmatizer function\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# declare the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fe5_quotes = []\n",
    "for quote in fe3_quotes:\n",
    "    words = word_tokenize(quote)\n",
    "    # stem each of the words for each of our quotes\n",
    "    lemma_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemma_quote = ''\n",
    "    for lemma_word in lemma_words:\n",
    "        lemma_quote += ' ' + lemma_word\n",
    "    fe5_quotes.append(lemma_quote)\n",
    "print(fe5_quotes)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
